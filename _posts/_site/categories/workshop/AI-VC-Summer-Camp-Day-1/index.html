<p>2018暑假辦在清大的AI VC Summer Camp紀錄，今天的內容主要：</p>

<p><a href="https://arxiv.org/pdf/1503.08895.pdf">投影片連結</a></p>

<h2 id="memn2n">MemN2N</h2>

<p><img src="https://i.imgur.com/UAa7fCN.png" alt="" /></p>

<p>place all inputs in the memory
和RNN不同的地方在於先把所有的input data存到memory中，
只控制memory的作用
但RNN是一次input 一筆data進來</p>

<p><img src="https://i.imgur.com/zjfl7iE.png" alt="" /></p>

<h3 id="advantages">Advantages</h3>
<p>1) More generic input format
(任何vector都可以input)
像是bag of words / image feature / feature position
2) out-of-order access to input data
3) less distracted by unimportant inputs
4) longer term memorization
5) no vanishing / exploding gradients 
(不會重複propagate W)</p>

<h2 id="disadvantages">Disadvantages</h2>

<p>讓model決定哪一段memory要access，
 而不是用來訓練weights</p>

<h2 id="end-to-end-memory-networks">End-to-End Memory Networks</h2>

<p>用bag-of-words來表示文字 (query embedding)
<img src="https://i.imgur.com/IFoWS0z.png" alt="" />
把input分成兩路餵進output memory和input memory中</p>

<p>input memory:計算 attention weights
second memory:access memory slots(用來訓練哪個位置的memory是需要被access的) decide which memory slot should be accessed</p>

<p>soft attention (使用softmax): probability of accessing memory slots</p>

<p>Dimension of memory / bag-of-words 必須在事前先決定好
Dimension of W:R^(|V|xD)</p>

<h3 id="multi-hop-reasoning">Multi-hop Reasoning</h3>
<p>對於整個input/output memory unit access多次</p>

<h4 id="weight-sharing">Weight sharing:</h4>
<p>1) adjacent (前一層的output W是下一層的input W)
2) layer-wise in/out embeddings are the same across all layers</p>

<h3 id="netowork實作中的技巧">Netowork實作中的技巧</h3>
<p>1) BOW(Setence representation)
2) Position encoding (Setence representation)
3) temportal encoding: change the order of sentence input will change the results, is a 需要被學習的 matrix，用於 encode input句子的順序
4) 在空的memory中加入10% random noise做initialization</p>

<p>joint training: 同時訓練多個模型
比較多hops有助於performance提升（K=3）</p>

<hr />

<h2 id="key-value-memory-networks-kv-memnn-neural-programming">Key-Value Memory Networks (KV-MemNN): Neural Programming</h2>

<p>https://arxiv.org/pdf/1606.03126.pdf</p>

<p>generalize the way context is stored in the memory
structured memories as key-value</p>

<h3 id="key-value-paired-memory">Key-Value paired memory</h3>
<p>key:用來決定哪個memory要access
value：實際上儲存的data（match the memory to the response）
key和value可以是words/ sentence / vectors</p>

<h3 id="key-hashing">Key Hashing</h3>
<p>given a question x, pre-select 一個小的subset for memory
任何的hashing function都可以用 (inverted idx / LSH)</p>

<h3 id="key-addressing">Key Addressing</h3>
<p>relevance probability of each memory slot to x</p>

<h3 id="value-reading">Value Reading</h3>
<p>taking avg of memory values</p>

<h3 id="如何建立-key-value-的關係">如何建立 key-value 的關係</h3>
<p>KV triple: subject relation object</p>

<hr />

<h2 id="neural-turing-machines">Neural Turing Machines</h2>
<p>https://arxiv.org/pdf/1410.5401.pdf</p>

<p>http://www.robots.ox.ac.uk/~tvg/publications/talks/NeuralTuringMachines.pdf</p>

<h3 id="rnn-is-enough">RNN is enough?</h3>
<p>如果只有幾個hidden state是否足以代表長的句子？
-&gt; 需要做memory augmentation
-&gt; extend the capability of NN by coupling to external memory</p>

<h3 id="artificial-working-memory">Artificial working memory</h3>
<p>using 額外的process和external memory 互動</p>

<p><img src="https://i.imgur.com/KuHuxfb.png" alt="" /></p>

<h3 id="blurry-read--write">Blurry Read / Write</h3>
<p>a set of parallel R/W heads
透過controller來控制memory</p>

<p>soft attention on memory highly sparsely</p>

<h3 id="content-addressing">Content Addressing</h3>
<p>根據內容決定要拿哪裡的memory而不是看storage的位置，It is typically used for high-speed storage and retrieval of fixed content
focuses attention on locations based on the similarity between their current values and values emitted by the controller.</p>

<h3 id="intepolation-for-location-based-addressing">Intepolation for Location-based addressing</h3>
<p>The location-based addressing mechanism is designed to facilitate both simple iteration across the locations of the memory and random-access jumps. It does so by implementing a rotational shift of a weighting.
gate = 1 : 使用目前的memory,忽略過去一個time step的
gate = 0 : 使用過去一個time step的memory</p>

<h3 id="convoliutional-shift">Convoliutional Shift</h3>
<p>allowed integers shift
（通常是找前後幾個time-step的memory）</p>

<h3 id="sharpening">Sharpening</h3>

<h2 id="memnn-vs-ntm">MemNN v.s. NTM</h2>
<p>MemNN: memory是static，只有關注如何retrive/read information from memory
NTM: 持續不斷寫入跟讀取，network是用來學習「何時要read/write memory」/ memory的interface會比較彈性，但結構就比較複雜</p>

<hr />

<h2 id="differentiable-neural-computers">Differentiable Neural Computers</h2>
<p><img src="https://i.imgur.com/VcdFMm5.png" alt="" /></p>

<p>有比較複雜的memory addressing mechanism，和NTM相比</p>
<ul>
  <li>加上 Dynamic memory allocation (writing)</li>
  <li>加上 Temporl memory linkage (reading)</li>
</ul>

<h3 id="reasons-of-dynamic-memory-allocation">Reasons of dynamic memory allocation</h3>
<p>1) 因為NTM只有在連續的memory blocks allocate memory，但這樣的話需要管理記憶體（否則資料會沒有ptr指向&amp;難處理）
2) 沒辦法釋放memory
3) location addressing 需要大量的連續記憶體</p>

<h3 id="reasons-of-temporal-memory-linkage">Reasons of Temporal memory linkage</h3>
<p>1) search complexity by time
2) preserving temporal order (track the order of writing)</p>

<h3 id="ntm和dnc的差異">NTM和DNC的差異</h3>
<p>1) DNC有更多的scalar gate用以計算weights間的內插值
2) DNC引入memory retention vector用以更新usage vector：
    a) 如果在前一個time step該memory的位置是write，則其retention weight應該比較大
    b) 如果在前一個time step該memory的位置是read，則其retention weight應該比較小</p>

<p><img src="https://i.imgur.com/eZ1GY93.png" alt="" /></p>

<h3 id="temporal-memory-linkage-linkage-matrix-l_t">Temporal Memory Linkage (Linkage matrix, L_{t})</h3>
<p>represent the writing order of memory slots</p>

<h4 id="write-weighting">Write weighting</h4>
<p>1) Content-based addressing
2) Dynamic memory allocation</p>

<p><img src="https://i.imgur.com/1AWbfGH.png" alt="" />
<img src="https://i.imgur.com/Ci2K0PG.png" alt="" /></p>

<h4 id="read-weighting">Read weighting</h4>
<p>1) Content-based addressing
2) Temporal memory linkage</p>

<p><img src="https://i.imgur.com/a8q8i7S.png" alt="" />
<img src="https://i.imgur.com/sLz8Blz.png" alt="" /></p>

